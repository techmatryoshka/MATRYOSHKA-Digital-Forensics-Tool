# Matryoshka SQLite Database Guide

Matryoshka relies on SQLite as its persistent storage backend to maintain forensic evidence integrity and enable advanced analysis capabilities. Here's everything you need to know about working with the database.

## Database Schema

### Table: `nested_findings`
Stores all discovered artifacts with forensic metadata:

```sql
CREATE TABLE nested_findings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,           -- ISO 8601 discovery time
    layer TEXT NOT NULL,               -- Which analysis layer found it
    artifact_type TEXT NOT NULL,       -- Type of artifact discovered
    location TEXT NOT NULL,            -- File path or network address
    description TEXT,                  -- Human-readable description
    evidence_hash TEXT,               -- SHA256/MD5 hash for integrity
    depth_score INTEGER,              -- Suspicion level (1-6)
    metadata TEXT                     -- JSON additional data
);
```

### Table: `analysis_sessions`
Tracks historical analysis runs:

```sql
CREATE TABLE analysis_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    start_time TEXT NOT NULL,         -- Analysis start timestamp
    end_time TEXT,                    -- Analysis completion time
    total_layers INTEGER DEFAULT 0,   -- Total artifacts found
    deepest_layer INTEGER DEFAULT 0   -- Deepest layer reached
);
```

## Practical SQLite Usage Examples

### 1. Basic Queries

**View recent findings:**
```sql
sqlite3 matryoshka.db "
SELECT timestamp, layer, artifact_type, location, depth_score 
FROM nested_findings 
ORDER BY timestamp DESC 
LIMIT 10;
"
```

**Find critical threats:**
```sql
sqlite3 matryoshka.db "
SELECT * FROM nested_findings 
WHERE depth_score >= 5 
ORDER BY depth_score DESC;
"
```

**Analysis session summary:**
```sql
sqlite3 matryoshka.db "
SELECT 
    DATE(start_time) as analysis_date,
    total_layers as artifacts_found,
    deepest_layer as max_layer_reached,
    CASE 
        WHEN deepest_layer >= 5 THEN 'CRITICAL'
        WHEN deepest_layer >= 3 THEN 'HIGH'
        ELSE 'MODERATE'
    END as threat_level
FROM analysis_sessions 
ORDER BY start_time DESC;
"
```

### 2. Forensic Timeline Analysis

**Create artifact timeline:**
```sql
sqlite3 matryoshka.db "
SELECT 
    DATE(timestamp) as date,
    TIME(timestamp) as time,
    layer,
    artifact_type,
    location,
    depth_score
FROM nested_findings 
WHERE date(timestamp) = date('now')
ORDER BY timestamp;
"
```

**Identify attack progression:**
```sql
sqlite3 matryoshka.db "
SELECT 
    layer,
    COUNT(*) as artifact_count,
    AVG(depth_score) as avg_suspicion,
    MAX(depth_score) as max_threat
FROM nested_findings 
GROUP BY layer 
ORDER BY 
    CASE layer
        WHEN 'surface_layer' THEN 1
        WHEN 'deletion_layer' THEN 2  
        WHEN 'process_layer' THEN 3
        WHEN 'network_shadows' THEN 4
        WHEN 'core_layer' THEN 5
    END;
"
```

### 3. Threat Intelligence Queries

**Find repeat locations (persistence indicators):**
```sql
sqlite3 matryoshka.db "
SELECT 
    location,
    COUNT(*) as occurrences,
    GROUP_CONCAT(DISTINCT artifact_type) as types,
    MAX(depth_score) as highest_threat,
    MIN(timestamp) as first_seen,
    MAX(timestamp) as last_seen
FROM nested_findings 
GROUP BY location 
HAVING occurrences > 1
ORDER BY occurrences DESC, highest_threat DESC;
"
```

**Identify attack patterns:**
```sql
sqlite3 matryoshka.db "
SELECT 
    artifact_type,
    COUNT(*) as frequency,
    AVG(depth_score) as avg_threat,
    GROUP_CONCAT(DISTINCT layer) as found_in_layers
FROM nested_findings 
GROUP BY artifact_type 
ORDER BY frequency DESC, avg_threat DESC;
"
```

**Time-based correlation:**
```sql
sqlite3 matryoshka.db "
SELECT 
    datetime(timestamp) as time,
    location,
    artifact_type,
    depth_score,
    LAG(datetime(timestamp)) OVER (ORDER BY timestamp) as prev_time,
    (julianday(timestamp) - julianday(LAG(timestamp) OVER (ORDER BY timestamp))) * 24 * 60 as minutes_since_prev
FROM nested_findings 
WHERE date(timestamp) = date('now')
ORDER BY timestamp;
"
```

## Advanced Analysis Scripts

### 1. Automated Threat Assessment

```bash
#!/bin/bash
# threat_assessment.sh

DB="matryoshka.db"

echo "=== MATRYOSHKA THREAT ASSESSMENT ==="
echo "Generated: $(date)"
echo

# Critical findings count
CRITICAL=$(sqlite3 $DB "SELECT COUNT(*) FROM nested_findings WHERE depth_score = 6;")
HIGH_RISK=$(sqlite3 $DB "SELECT COUNT(*) FROM nested_findings WHERE depth_score = 5;")
MODERATE=$(sqlite3 $DB "SELECT COUNT(*) FROM nested_findings WHERE depth_score >= 3 AND depth_score <= 4;")

echo "Threat Summary:"
echo "  Critical (6): $CRITICAL"
echo "  High Risk (5): $HIGH_RISK" 
echo "  Moderate (3-4): $MODERATE"
echo

# Show critical findings
if [ $CRITICAL -gt 0 ]; then
    echo "=== CRITICAL FINDINGS ==="
    sqlite3 -header -column $DB "
    SELECT timestamp, location, description 
    FROM nested_findings 
    WHERE depth_score = 6 
    ORDER BY timestamp DESC;
    "
    echo
fi

# Layer penetration analysis
echo "=== LAYER PENETRATION ==="
sqlite3 -header -column $DB "
SELECT 
    layer,
    COUNT(*) as findings,
    MAX(depth_score) as max_threat
FROM nested_findings 
GROUP BY layer 
ORDER BY max_threat DESC;
"
```

### 2. Evidence Export Script

```python
#!/usr/bin/env python3
# export_evidence.py

import sqlite3
import json
import csv
import sys
from datetime import datetime

def export_to_json(db_path, output_file):
    """Export findings to JSON for SIEM integration"""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    
    cursor = conn.execute("""
        SELECT * FROM nested_findings 
        ORDER BY timestamp DESC
    """)
    
    findings = []
    for row in cursor:
        finding = dict(row)
        # Parse metadata JSON if present
        if finding['metadata']:
            try:
                finding['metadata'] = json.loads(finding['metadata'])
            except:
                pass
        findings.append(finding)
    
    with open(output_file, 'w') as f:
        json.dump(findings, f, indent=2, default=str)
    
    conn.close()
    print(f"Exported {len(findings)} findings to {output_file}")

def export_to_csv(db_path, output_file):
    """Export findings to CSV for spreadsheet analysis"""
    conn = sqlite3.connect(db_path)
    
    cursor = conn.execute("""
        SELECT 
            timestamp,
            layer,
            artifact_type,
            location,
            description,
            depth_score,
            evidence_hash
        FROM nested_findings 
        ORDER BY timestamp DESC
    """)
    
    with open(output_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Timestamp', 'Layer', 'Type', 'Location', 'Description', 'Depth', 'Hash'])
        writer.writerows(cursor)
    
    conn.close()
    print(f"Exported findings to {output_file}")

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("Usage: python export_evidence.py <db_path> <format> <output_file>")
        print("Format: json or csv")
        sys.exit(1)
    
    
    if format_type == 'json':
        export_to_json(db_path, output_file)
    elif format_type == 'csv':
        export_to_csv(db_path, output_file)
    else:
        print("Invalid format. Use 'json' or 'csv'")
```

### 3. Database Maintenance

```sql
-- Clean old findings (older than 30 days)
DELETE FROM nested_findings 
WHERE datetime(timestamp) < datetime('now', '-30 days');

-- Optimize database
VACUUM;

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS idx_timestamp ON nested_findings(timestamp);
CREATE INDEX IF NOT EXISTS idx_depth_score ON nested_findings(depth_score);
CREATE INDEX IF NOT EXISTS idx_layer ON nested_findings(layer);
CREATE INDEX IF NOT EXISTS idx_location ON nested_findings(location);

-- Database statistics
SELECT 
    'Total Findings' as metric,
    COUNT(*) as value
FROM nested_findings
UNION ALL
SELECT 
    'Analysis Sessions',
    COUNT(*)
FROM analysis_sessions
UNION ALL
SELECT 
    'Database Size (MB)',
    ROUND(page_count * page_size / 1024.0 / 1024.0, 2)
FROM pragma_page_count(), pragma_page_size();
```

## Integration Examples

### 1. SIEM Integration (Splunk)

```bash
# Export and send to Splunk
python export_evidence.py matryoshka.db json findings.json
curl -X POST \
  -H "Authorization: Bearer $SPLUNK_TOKEN" \
  -H "Content-Type: application/json" \
  -d @findings.json \
  "$SPLUNK_HEC_URL/services/collector/event"
```

### 2. Elasticsearch Integration

```python
import json
import sqlite3
from elasticsearch import Elasticsearch

def send_to_elasticsearch(db_path, es_host):
    es = Elasticsearch([es_host])
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    
    cursor = conn.execute("SELECT * FROM nested_findings WHERE date(timestamp) = date('now')")
    
    for row in cursor:
        doc = dict(row)
        doc['@timestamp'] = doc['timestamp']
        
        es.index(
            index='matryoshka-findings',
            body=doc
        )
    
    conn.close()

# Usage
send_to_elasticsearch('matryoshka.db', 'localhost:9200')
```

### 3. Custom Alerting

```python
#!/usr/bin/env python3
# alert_monitor.py

import sqlite3
import smtplib
from email.mime.text import MIMEText
import time

def check_for_critical_findings(db_path):
    conn = sqlite3.connect(db_path)
    
    # Check for new critical findings in last 5 minutes
    cursor = conn.execute("""
        SELECT COUNT(*) FROM nested_findings 
        WHERE depth_score = 6 
        AND datetime(timestamp) > datetime('now', '-5 minutes')
    """)
    
    critical_count = cursor.fetchone()[0]
    conn.close()
    
    return critical_count

def send_alert(count):
    msg = MIMEText(f"CRITICAL ALERT: {count} critical threats detected by Matryoshka!")
    msg['Subject'] = 'Matryoshka Security Alert'
    msg['From'] = 'matryoshka@company.com'
    msg['To'] = 'security-team@company.com'
    
    # Configure your SMTP settings
    smtp = smtplib.SMTP('localhost')
    smtp.send_message(msg)
    smtp.quit()

# Monitor loop
while True:
    critical_findings = check_for_critical_findings('matryoshka.db')
    if critical_findings > 0:
        send_alert(critical_findings)
    
    time.sleep(300)  # Check every 5 minutes
```

## Database Tools

### Quick CLI Access
```bash
# Interactive SQL shell
sqlite3 matryoshka.db

# One-liner queries
alias matryoshka-db="sqlite3 matryoshka.db"
matryoshka-db "SELECT COUNT(*) FROM nested_findings;"

# Pretty output
sqlite3 -header -column matryoshka.db "SELECT * FROM nested_findings LIMIT 5;"
```

### Backup and Recovery
```bash
# Backup database
sqlite3 matryoshka.db ".backup matryoshka_backup_$(date +%Y%m%d).db"

# Export to SQL
sqlite3 matryoshka.db ".dump" > matryoshka_export.sql

# Restore from backup
sqlite3 matryoshka_restored.db ".restore matryoshka_backup.db"
```

This SQLite integration makes Matryoshka incredibly powerful for forensic analysis, threat hunting, and integration with existing security infrastructure!
    # Matryoshka SQLite Database Guide

Matryoshka uses SQLite as its persistent storage backend to maintain forensic evidence integrity and enable advanced analysis capabilities. Here's everything you need to know about working with the database.

## Database Schema

### Table: `nested_findings`
Stores all discovered artifacts with forensic metadata:

```sql
CREATE TABLE nested_findings (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,           -- ISO 8601 discovery time
    layer TEXT NOT NULL,               -- Which analysis layer found it
    artifact_type TEXT NOT NULL,       -- Type of artifact discovered
    location TEXT NOT NULL,            -- File path or network address
    description TEXT,                  -- Human-readable description
    evidence_hash TEXT,               -- SHA256/MD5 hash for integrity
    depth_score INTEGER,              -- Suspicion level (1-6)
    metadata TEXT                     -- JSON additional data
);
```

### Table: `analysis_sessions`
Tracks historical analysis runs:

```sql
CREATE TABLE analysis_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    start_time TEXT NOT NULL,         -- Analysis start timestamp
    end_time TEXT,                    -- Analysis completion time
    total_layers INTEGER DEFAULT 0,   -- Total artifacts found
    deepest_layer INTEGER DEFAULT 0   -- Deepest layer reached
);
```

## Practical SQLite Usage Examples

### 1. Basic Queries

**View recent findings:**
```sql
sqlite3 matryoshka.db "
SELECT timestamp, layer, artifact_type, location, depth_score 
FROM nested_findings 
ORDER BY timestamp DESC 
LIMIT 10;
"
```

**Find critical threats:**
```sql
sqlite3 matryoshka.db "
SELECT * FROM nested_findings 
WHERE depth_score >= 5 
ORDER BY depth_score DESC;
"
```

**Analysis session summary:**
```sql
sqlite3 matryoshka.db "
SELECT 
    DATE(start_time) as analysis_date,
    total_layers as artifacts_found,
    deepest_layer as max_layer_reached,
    CASE 
        WHEN deepest_layer >= 5 THEN 'CRITICAL'
        WHEN deepest_layer >= 3 THEN 'HIGH'
        ELSE 'MODERATE'
    END as threat_level
FROM analysis_sessions 
ORDER BY start_time DESC;
"
```

### 2. Forensic Timeline Analysis

**Create artifact timeline:**
```sql
sqlite3 matryoshka.db "
SELECT 
    DATE(timestamp) as date,
    TIME(timestamp) as time,
    layer,
    artifact_type,
    location,
    depth_score
FROM nested_findings 
WHERE date(timestamp) = date('now')
ORDER BY timestamp;
"
```

**Identify attack progression:**
```sql
sqlite3 matryoshka.db "
SELECT 
    layer,
    COUNT(*) as artifact_count,
    AVG(depth_score) as avg_suspicion,
    MAX(depth_score) as max_threat
FROM nested_findings 
GROUP BY layer 
ORDER BY 
    CASE layer
        WHEN 'surface_layer' THEN 1
        WHEN 'deletion_layer' THEN 2  
        WHEN 'process_layer' THEN 3
        WHEN 'network_shadows' THEN 4
        WHEN 'core_layer' THEN 5
    END;
"
```

### 3. Threat Intelligence Queries

**Find repeat locations (persistence indicators):**
```sql
sqlite3 matryoshka.db "
SELECT 
    location,
    COUNT(*) as occurrences,
    GROUP_CONCAT(DISTINCT artifact_type) as types,
    MAX(depth_score) as highest_threat,
    MIN(timestamp) as first_seen,
    MAX(timestamp) as last_seen
FROM nested_findings 
GROUP BY location 
HAVING occurrences > 1
ORDER BY occurrences DESC, highest_threat DESC;
"
```

**Identify attack patterns:**
```sql
sqlite3 matryoshka.db "
SELECT 
    artifact_type,
    COUNT(*) as frequency,
    AVG(depth_score) as avg_threat,
    GROUP_CONCAT(DISTINCT layer) as found_in_layers
FROM nested_findings 
GROUP BY artifact_type 
ORDER BY frequency DESC, avg_threat DESC;
"
```

**Time-based correlation:**
```sql
sqlite3 matryoshka.db "
SELECT 
    datetime(timestamp) as time,
    location,
    artifact_type,
    depth_score,
    LAG(datetime(timestamp)) OVER (ORDER BY timestamp) as prev_time,
    (julianday(timestamp) - julianday(LAG(timestamp) OVER (ORDER BY timestamp))) * 24 * 60 as minutes_since_prev
FROM nested_findings 
WHERE date(timestamp) = date('now')
ORDER BY timestamp;
"
```

## Advanced Analysis Scripts

### 1. Automated Threat Assessment

```bash
#!/bin/bash
# threat_assessment.sh

DB="matryoshka.db"

echo "=== MATRYOSHKA THREAT ASSESSMENT ==="
echo "Generated: $(date)"
echo

# Critical findings count
CRITICAL=$(sqlite3 $DB "SELECT COUNT(*) FROM nested_findings WHERE depth_score = 6;")
HIGH_RISK=$(sqlite3 $DB "SELECT COUNT(*) FROM nested_findings WHERE depth_score = 5;")
MODERATE=$(sqlite3 $DB "SELECT COUNT(*) FROM nested_findings WHERE depth_score >= 3 AND depth_score <= 4;")

echo "Threat Summary:"
echo "  Critical (6): $CRITICAL"
echo "  High Risk (5): $HIGH_RISK" 
echo "  Moderate (3-4): $MODERATE"
echo

# Show critical findings
if [ $CRITICAL -gt 0 ]; then
    echo "=== CRITICAL FINDINGS ==="
    sqlite3 -header -column $DB "
    SELECT timestamp, location, description 
    FROM nested_findings 
    WHERE depth_score = 6 
    ORDER BY timestamp DESC;
    "
    echo
fi

# Layer penetration analysis
echo "=== LAYER PENETRATION ==="
sqlite3 -header -column $DB "
SELECT 
    layer,
    COUNT(*) as findings,
    MAX(depth_score) as max_threat
FROM nested_findings 
GROUP BY layer 
ORDER BY max_threat DESC;
"
```

### 2. Evidence Export Script

```python
#!/usr/bin/env python3
# export_evidence.py

import sqlite3
import json
import csv
import sys
from datetime import datetime

def export_to_json(db_path, output_file):
    """Export findings to JSON for SIEM integration"""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    
    cursor = conn.execute("""
        SELECT * FROM nested_findings 
        ORDER BY timestamp DESC
    """)
    
    findings = []
    for row in cursor:
        finding = dict(row)
        # Parse metadata JSON if present
        if finding['metadata']:
            try:
                finding['metadata'] = json.loads(finding['metadata'])
            except:
                pass
        findings.append(finding)
    
    with open(output_file, 'w') as f:
        json.dump(findings, f, indent=2, default=str)
    
    conn.close()
    print(f"Exported {len(findings)} findings to {output_file}")

def export_to_csv(db_path, output_file):
    """Export findings to CSV for spreadsheet analysis"""
    conn = sqlite3.connect(db_path)
    
    cursor = conn.execute("""
        SELECT 
            timestamp,
            layer,
            artifact_type,
            location,
            description,
            depth_score,
            evidence_hash
        FROM nested_findings 
        ORDER BY timestamp DESC
    """)
    
    with open(output_file, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Timestamp', 'Layer', 'Type', 'Location', 'Description', 'Depth', 'Hash'])
        writer.writerows(cursor)
    
    conn.close()
    print(f"Exported findings to {output_file}")

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("Usage: python export_evidence.py <db_path> <format> <output_file>")
        print("Format: json or csv")
        sys.exit(1)
    
    db_path, format_type, output_file = sys.argv[1:4]
    
    if format_type == 'json':
        export_to_json(db_path, output_file)
    elif format_type == 'csv':
        export_to_csv(db_path, output_file)
    else:
        print("Invalid format. Use 'json' or 'csv'")
```

### 3. Database Maintenance

```sql
-- Clean old findings (older than 30 days)
DELETE FROM nested_findings 
WHERE datetime(timestamp) < datetime('now', '-30 days');

-- Optimize database
VACUUM;

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS idx_timestamp ON nested_findings(timestamp);
CREATE INDEX IF NOT EXISTS idx_depth_score ON nested_findings(depth_score);
CREATE INDEX IF NOT EXISTS idx_layer ON nested_findings(layer);
CREATE INDEX IF NOT EXISTS idx_location ON nested_findings(location);

-- Database statistics
SELECT 
    'Total Findings' as metric,
    COUNT(*) as value
FROM nested_findings
UNION ALL
SELECT 
    'Analysis Sessions',
    COUNT(*)
FROM analysis_sessions
UNION ALL
SELECT 
    'Database Size (MB)',
    ROUND(page_count * page_size / 1024.0 / 1024.0, 2)
FROM pragma_page_count(), pragma_page_size();
```

## 🔗 Integration Examples

### 1. SIEM Integration (Splunk)

```bash
# Export and send to Splunk
python export_evidence.py matryoshka.db json findings.json
curl -X POST \
  -H "Authorization: Bearer $SPLUNK_TOKEN" \
  -H "Content-Type: application/json" \
  -d @findings.json \
  "$SPLUNK_HEC_URL/services/collector/event"
```

### 2. Elasticsearch Integration

```python
import json
import sqlite3
from elasticsearch import Elasticsearch

def send_to_elasticsearch(db_path, es_host):
    es = Elasticsearch([es_host])
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    
    cursor = conn.execute("SELECT * FROM nested_findings WHERE date(timestamp) = date('now')")
    
    for row in cursor:
        doc = dict(row)
        doc['@timestamp'] = doc['timestamp']
        
        es.index(
            index='matryoshka-findings',
            body=doc
        )
    
    conn.close()

# Usage
send_to_elasticsearch('matryoshka.db', 'localhost:9200')
```

### 3. Custom Alerting

```python
#!/usr/bin/env python3
# alert_monitor.py

import sqlite3
import smtplib
from email.mime.text import MIMEText
import time

def check_for_critical_findings(db_path):
    conn = sqlite3.connect(db_path)
    
    # Check for new critical findings in last 5 minutes
    cursor = conn.execute("""
        SELECT COUNT(*) FROM nested_findings 
        WHERE depth_score = 6 
        AND datetime(timestamp) > datetime('now', '-5 minutes')
    """)
    
    critical_count = cursor.fetchone()[0]
    conn.close()
    
    return critical_count

def send_alert(count):
    msg = MIMEText(f"CRITICAL ALERT: {count} critical threats detected by Matryoshka!")
    msg['Subject'] = 'Matryoshka Security Alert'
    msg['From'] = 'matryoshka@company.com'
    msg['To'] = 'security-team@company.com'
    
    # Configure your SMTP settings
    smtp = smtplib.SMTP('localhost')
    smtp.send_message(msg)
    smtp.quit()

# Monitor loop
while True:
    critical_findings = check_for_critical_findings('matryoshka.db')
    if critical_findings > 0:
        send_alert(critical_findings)
    
    time.sleep(300)  # Check every 5 minutes
```

## Database Tools

### Quick CLI Access
```bash
# Interactive SQL shell
sqlite3 matryoshka.db

# One-liner queries
alias matryoshka-db="sqlite3 matryoshka.db"
matryoshka-db "SELECT COUNT(*) FROM nested_findings;"

# Pretty output
sqlite3 -header -column matryoshka.db "SELECT * FROM nested_findings LIMIT 5;"
```

### Backup and Recovery
```bash
# Backup database
sqlite3 matryoshka.db ".backup matryoshka_backup_$(date +%Y%m%d).db"

# Export to SQL
sqlite3 matryoshka.db ".dump" > matryoshka_export.sql

# Restore from backup
sqlite3 matryoshka_restored.db ".restore matryoshka_backup.db"
```

This SQLite integration makes Matryoshka incredibly powerful for forensic analysis, threat hunting, and integration with existing security infrastructure!
